{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data and store in a dataframe\n",
    "\n",
    "df = pd.read_csv('winequality-red.csv') # change this line to your own data file as required\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a heatmap of correlation between columns\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.set(font_scale=0.8)\n",
    "\n",
    "mask = np.zeros_like(df.corr())\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.heatmap(df.corr(), mask=mask, annot=True, cmap='RdYlGn', fmt='.2f', vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate our data into our inputs, X, and output, y\n",
    "\n",
    "X = df[['volatile acidity','total sulfur dioxide', 'alcohol']] # change these to column names in your own data\n",
    "y = df['quality'] # change this to column name in your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate our X and y into train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our linear regression object and train the model \n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the intercept and coefficients for our model\n",
    "\n",
    "feature_df = pd.DataFrame({'intercept': lr.intercept_, \n",
    "                           'coefficients': lr.coef_,}, \n",
    "                           index = X_train.columns)\n",
    "\n",
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the R^2 score for how well our model performed with our training data\n",
    "# higher = better!\n",
    "\n",
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and then the R^2 score for how well the model performs on our test data\n",
    "\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our model to make some predictions and then see how they compare to the known answers we have\n",
    "# in our test data with Mean Absolute Error (MAE)\n",
    "# lower = better!\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "mean_absolute_error(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and similar with Root Mean Squared Error (RMSE)\n",
    "\n",
    "np.sqrt(mean_squared_error(preds,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate your model for an average R^2 score with different testing sets - still higher = better!\n",
    "\n",
    "cross_val = cross_val_score(lr, X, y, cv = 5)\n",
    "\n",
    "cross_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an alternative, you can also create and fit your model with the Statsmodels package,\n",
    "# and see a summary of your model results\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "model= sm.OLS(y, X) \n",
    "\n",
    "res = model.fit()\n",
    "\n",
    "res.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, classification_report, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data and store in a dataframe\n",
    "\n",
    "college = pd.read_csv('College_data.csv') # change to your own data file as required\n",
    "\n",
    "# we create an extra column for our above or below average label - you may need to do this differently for your\n",
    "# own data, or not at all if the label column you want to predict already exists in your data\n",
    "college['above_average'] = college['Grad.Rate'].apply(lambda x: 1 if x >= college['Grad.Rate'].mean() else 0)\n",
    "\n",
    "college.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate our data into our input columns, X, and our output column, y\n",
    "\n",
    "X = college[['Top10perc']] # change these to column names in your own data\n",
    "y = college['above_average'] # change this to a column name in your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, train_size = 0.8, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our logistic model and train it on our training data\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have a new piece of data we want to predict, we can just enter it to .predict\n",
    "# and get a predicted label back, 0 or 1\n",
    "\n",
    "new_uni_top_10_perc = 22\n",
    "\n",
    "logreg.predict([[new_uni_top_10_perc]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also look at the probabilities assigned to each label for that new data with\n",
    "# .predict_proba, which might be easier to read in a dataframe\n",
    "\n",
    "pd.DataFrame(logreg.predict_proba([[new_uni_top_10_perc]]), index = [\"Prob. of Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create a dataframe of new points we want to feed into the model to make predictions with,\n",
    "# and use .map to put some nice human-readable labels onto our 0 or 1 predicted values\n",
    "\n",
    "new_data = [[20], [22], [25], [40]]\n",
    "\n",
    "preds_df = pd.DataFrame({'Top10Perc': new_data, 'prediction': logreg.predict(new_data)})\n",
    "\n",
    "preds_df['label'] = preds_df.prediction.map({0: 'Below', 1: 'Above'})\n",
    "\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is an example of everything from above, but this time with multiple columns in X\n",
    "# notice we need to just slightly tweak how we setup the dataframe of new data we want\n",
    "# to predict to have the same column names as in X\n",
    "\n",
    "X = college[['Top10perc', 'Top25perc']] # change these to column names in your own data\n",
    "y = college['above_average'] # change this to a column name in your own data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, train_size = 0.8, random_state = 100)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "new_data = pd.DataFrame({'Top10perc': [20, 22, 25, 40], 'Top25perc': [30, 32, 35, 50]}) # change these to match your columns names in X, and enter the list of new data you want to predict\n",
    "new_data['predictions'] = logreg.predict(new_data)\n",
    "\n",
    "new_data['label'] = new_data['predictions'].map({0: 'Below', 1: 'Above'})\n",
    "\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how well our model performs as a % of correct predictions on our training data\n",
    "# higher = better!\n",
    "\n",
    "logreg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how well our model performs as a % of correct predictions on our testing data\n",
    "# higher = better!\n",
    "\n",
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how well we could guess the label if we just randomly guessed 0 or 1 for everything\n",
    "# if our model scores above are higher than this, the model is performing better than a guess!\n",
    "\n",
    "college['above_average'].value_counts(normalize = True).max() # change dataframe and column name to your own y column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple confusion matrix to see the true and false positives and negatives in our model\n",
    "# see where rows and columns align - 0 and 0, 1 and 1 - for true negatives and positives\n",
    "# see where they don't - 0 and 1, 1 and 0 - for false negatives and positives\n",
    "# higher true and lower false = better!\n",
    "\n",
    "preds = logreg.predict(X_train)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_train, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also assess this with a precision score to see how often a prediction is correct\n",
    "# higher = better!\n",
    "\n",
    "precision_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a recall score to see how often a true value is predicted correctly\n",
    "# higher = better!\n",
    "\n",
    "recall_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see a summary of these with the classification report\n",
    "\n",
    "classification_report(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a receiver operating characteristic (ROC) curve to visually assess our model\n",
    "# the further our blue line curves to the top left, the more true positives there are, the better the model\n",
    "# is performing\n",
    "\n",
    "y_pred_proba = logreg.predict_proba(X_train)[::,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train,  y_pred_proba, drop_intermediate = False)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.plot([0, 1], [0, 1], color='lightgrey', lw=1, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and see this as an area under the curve (AUC-ROC) score\n",
    "# higher = better!\n",
    "\n",
    "roc_auc_score(y_train, y_pred_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
